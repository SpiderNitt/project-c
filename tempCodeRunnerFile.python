import lightning as L
from lightning.pytorch import LightningDataModule
import torch
import wandb
from lightning.pytorch.loggers import WandbLogger
from lightning.pytorch.callbacks import Callback, ModelSummary, LearningRateMonitor
from omegaconf import OmegaConf
from lightning.pytorch.tuner import Tuner
import torch.utils.data as data
from segment_anything import sam_model_registry
from segment_anything.modeling import CamoSam
from lightning.pytorch.utilities.rank_zero import rank_zero_only, rank_zero_warn

from dataloaders.vos_dataset import get_loader
from inference import infer

import os
torch.set_float32_matmul_precision('medium')

# DAVIS
config = {
    "description": "Positional embed before cross attention. No sparse embeddings. Pos embed wt has shape (256)",
    "precision": "32",
    "num_devices": 1,
    "num_epochs": 100,
    "save_log_weights_interval": 20,
    "metric_train_eval_interval": 20,
    "model_checkpoint_at": "checkpoints",
    "img_size": 1024,
    "out_dir": "/",
    "focal_wt": 20,
    "num_tokens": 1,
    "opt": {
        "learning_rate": 4e-4, #1e-4
        "auto_lr": False,
        "weight_decay": 1e-4,
        "decay_factor": 1/2,
        "steps": [100, 250],
    },
    "model": {
        "type": "vit_l",
        "checkpoint": "sam_vit_l_0b3195.pth",
        "requires_grad": {
            "image_encoder": False,
            "prompt_encoder": False,
            "mask_decoder": False,
            "propagation_module": True,
        },
    },
    "dataset": {
        "name": "davis",
        "root_dir": "DAVIS-evaluation/data/",
        "train_split": None, # Only for MoCA
        "stage1": False,
        "batch_size": 4,
        "max_num_obj": 3,
        "num_frames": 3,
        "max_jump": 5,
        "num_workers": 4,
        "pin_memory": False,
        "persistent_workers": True,
    },
}
cfg = OmegaConf.create(config)

def hi(config):
    config.dataset.batch_size = 1
hi(cfg)

print(cfg)